{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "The classification models are used to predict discrete outcomes `(y)` using indepent variables `(x)`.\n",
    "The dependent variable is always a **class** or a **category**.\n",
    "\n",
    "For instance:\n",
    "\n",
    "# Fruit Classification \n",
    "\n",
    "| Fruit              | Color (x1) | Weight (g) (x2) | Taste (x3) | Edible (y) | Predicted Edible (Å·) |\n",
    "|--------------------|------------|-----------------|------------|------------|----------------------|\n",
    "| Apple              | Red        | 150             | Sweet      | Yes        | Yes                  |\n",
    "| Banana             | Yellow     | 120             | Sweet      | Yes        | Yes                  |\n",
    "| Lemon              | Green      | 80              | Sour       | Yes        | No                   |\n",
    "| Orange             | Orange     | 130             | Sweet      | Yes        | Yes                  |\n",
    "| Grape              | Purple     | 5               | Sweet      | Yes        | Yes                  |\n",
    "| Tomato             | Red        | 100             | Umami      | Yes        | No                   |\n",
    "| Belladonna         | Black      | 5               | Sweet      | No         | No                   |\n",
    "| Amanita Mushroom   | White      | 30              | Bitter     | No         | Yes                  |\n",
    "| Kiwi               | Brown      | 75              | Sweet      | Yes        | Yes                  |\n",
    "| Papaya             | Orange     | 500             | Sweet      | Yes        | Yes                  |\n",
    "| Pear               | Green      | 180             | Sweet      | Yes        | No                   |\n",
    "| Strychnine Nut     | Brown      | 10              | Bitter     | No         | No                   |\n",
    "| Blackberry         | Black      | 5               | Sweet      | Yes        | Yes                  |\n",
    "\n",
    "## Classification metrics\n",
    "\n",
    "There are many ways to evaluate a classification algorithm based.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "It is the fraction of correct predictions made by the machine learning model.\n",
    "<center>\n",
    "\n",
    "\n",
    "`number of correct predicions / total number of observations `\n",
    "\n",
    "`9 / 13 =  0.69`\n",
    "</center>\n",
    "\n",
    "\n",
    "### Precision\n",
    "\n",
    "Precision is a metric used to evaluate the accuracy of a classification model. It is defined as the ratio of true positive predictions to the total number of positive predictions (both true and false positives).\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **True Positives (TP)**: The number of correctly predicted positive instances.\n",
    "- **False Positives (FP)**: The number of instances incorrectly predicted as positive.\n",
    "\n",
    "High precision indicates a low number of false positives, which means the model is good at predicting positive instances correctly.\n",
    "\n",
    "<center>\n",
    "\n",
    "`number of true_positives / #true_positives + #false_positives`\n",
    "\n",
    "`7 / 7 + 1 = 0.875`\n",
    "</center>\n",
    "\n",
    "\n",
    "### Recall\n",
    "\n",
    "It is used to calculate the quality of negative predictions made by the model. It is defined as the ratio of true positive predictions to the total number of actual positive instances (both true positives and false negatives).\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **True Negative (TN)**: The number of correctly predicted negative instances.\n",
    "- **False Negatives (FN)**: The number of positive instances incorrectly predicted as negative.\n",
    "\n",
    "High recall indicates a low number of false negatives, which means the model is good at identifying positive instances.\n",
    "\n",
    "<center>\n",
    "\n",
    "`number of true_positives / #true_positives + #false_negatives`\n",
    "\n",
    "`7 / 7 + 3 = 0.7`\n",
    "</center>\n",
    "\n",
    "## Mistakes \n",
    "\n",
    "### Underfitting:\n",
    "\n",
    "Underfitting occurs when a model is too simple to capture the complexity of the data, resulting in poor performance on both training and test data.\n",
    "\n",
    "### Overfitting :\n",
    "\n",
    "Overfitting happens when a model is overly complex and fits too closely to the details and noise in the training data, leading to excellent performance on training data but poor performance on test data.\n",
    "\n",
    "## Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = churn_df.drop(\"churn\", axis=1).values\n",
    "y = churn_df[\"churn\"].values\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(knn.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
